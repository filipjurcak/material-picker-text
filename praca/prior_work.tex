\chapter{Prior work}
\label{kap:prior}

In this section, we would like to point out research that was done related to the two problems we are trying to solve: inverse rendering and material segmentation.

\section{Inverse rendering from a single image}
As inverse rendering of a scene is difficult, previous research in the field focused either on subproblems of this problem (like inverse rendering of an object instead of a whole scene), or the research focused on estimation of a small number of properties of a scene \cite{li-inverse-rendering} \cite{sengupta-inverse-rendering} or a small number of materials \cite{material-recognition}.
\newline
To estimate intrinsic characteristics of an image authors in \cite{li-inverse-rendering} \cite{sengupta-inverse-rendering} used neural networks. To obtain the data for training, they augmented SUNCG dataset \cite{song2016ssc} by mapping photorealistic materials to geometries in this dataset or completely re-render images by using physically based renderer, as the original dataset was rendered only with OpenGL using Phong BRDF model and does not look realistic.
\newline
In \cite{sengupta-inverse-rendering}, authors proposed pipeline for estimating diffuse albedo, environment map and normals from a single image by using Inverse Rendering Network (IRN) and combination of two modules - direct renderer for computing direct illumination and Residual Appearance Renderer (RAR) for computing shading and reflections - to re-synthesize the input image from estimated components and to learn from real images where ground-truth data is not available. To train IRN to correctly predict environment map, they had to generate ground-truth data, as the environment map that the scene was rendered with was used as exterior lighting, which does not reflect illumination inside the scene. To address this issue, they also trained neural net (EnvMap net) to predict best average environment map for the whole scene (including illumination inside the scene), which they then set as their ground-truth for this parameter of IRN. The environment map predicted by IRN was then used in the direct renderer to approximate incoming illumination using numerical quadrature.
\newline
Different approach was presented in \cite{li-inverse-rendering}. In this paper, authors were able to predict diffuse albedo, normals, specular roughness, depth, and spatially-varying lighting, which is a technique for estimating per-pixel illumination. Obtaining such data unwisely is computationally expensive and memory consuming, thus they resolved to use spherical Gaussian lobes, that preserve all lighting frequencies but require far less parameters to store. This very detailed pre-computed irradiance enabled them to include differentiable renderer in their pipeline to simulate image creation process without any rendering related code written by the authors. Due to this precise estimation of parameters, state-of-the-art object insertion and material editing were made possible.
\section{Material Classification and Segmentation}
Material segmentation is especially challenging, as real-world materials have a rich texture, and the final look of the material is a combination of many scene properties like lighting, depth, normals, and so on.
\newline
There exists a large number of classifiers for classifying images into classes (like dogs, cats, etc.): e.g.\ AlexNet \cite{alexnet}, VGG \cite{vgg} and GoogLeNet \cite{googlenet}. These classifiers take an input image and their output is per-class probability of the object in the image belonging to that class. Most used approach to image segmentation we found was the use of transfer learning on models pre-trained as classifiers \cite{fcn} \cite{material-recognition}. Transfer learning is method for re-using parts of already trained model (and possibly change the output layers), and retrain only those layers that were not taken from the pre-trained model.
\newline
In \cite{fcn}, authors removed final classification layer and used several upsampling layers to output 21 feature maps of the same size as input image. These 21 maps represented per pixel probability of the pixel belonging to 21 classes they had in the dataset. To get the final image they had to apply post-processing by taking per-pixel maximum over these feature maps, with index of the map that contained maximum assigned as the final value. It is worth noting that the new model was trained on the exact same dataset as the pre-trained model.
\newline
On the other hand, Bell et al. \cite{material-recognition} introduced completely new and larger dataset with 23 material categories on which they fine-tuned a pre-trained model. Authors thus proved that transfer learning  also works on different dataset than it was originally trained on, at least for image segmentation.
\newline
Unsurprisingly, the deeper the trained model was, the better it performed, with either GoogLeNet (22 layers) or VGG (16 layers) as winners in both publications.
\newline
After introduction of residual networks, the state-of-the-art network for segmentation became DeepLab \cite{deeplab}, taking advantage of its unprecedented depth - model that was retrained had more than 100 layers.