\chapter*{Conclusion}
\addcontentsline{toc}{chapter}{Conclusion} % rucne pridanie do obsahu
\markboth{Conclusion}{Conclusion} % vyriesenie hlaviciek

In this thesis, we presented a method for the per-pixel estimation of material properties in the image by training deep neural networks. We demonstrated that deep neural networks are powerful learning representations that can learn useful priors, even when it comes to such unconstrained problems like inverse rendering. Our pipeline collectively estimates diffuse and specular albedo, surface normals, glossiness, view vector, and illumination, alongside per-pixel material segmentation, from a single image. These properties are then used to re-synthesize the input image from its components. We provide an implementation of our direct render function that makes this synthesis possible.
\newline
We acknowledge, however, that our solution is not perfect and there is a significant amount of work to be done in the future, as our method infers only a handful of scene characteristics, which is not enough for most real-life use cases. First, we believe that our results could improve dramatically if we had more data in our dataset, thus leading to better generalization across all models. Because of that, enlarging our dataset will be our foremost goal.
\newline
To make our tool accessible to artists, we want to create a plugin for 3ds Max as a wrapper for our trained models. By integrating our models directly into the program, the estimated properties of a user-specified object in an image would be transferred onto the 3D model created in 3ds Max, also specified by the user.
\newline
One of the potential improvements to our work is to predict spatially varying lighting, approach very similar to that proposed in \cite{li-inverse-rendering}. Currently, we have only one environment map per image, which is a very rough approximation of lighting in the scene. To improve this estimate, we could generate an environment map (or some less parameter dependant representation, like spherical Gaussians) at every pixel in the scene, which is doable, as we know (from V-Ray) what is the first intersection point in the scene from the camera view.
\newline
The next step to further simplify the process of setting material appearance is to enable texture transfer, as having good texture for the material is equally important for final material looks as setting correct values for material parameters. Previous work on texture transfer was only limited to some classes of objects \cite{texture-transfer}, which is not satisfactory for our use case. We admit that this problem is extremely challenging, but we want to try nevertheless.
\newline
Altogether, we are convinced that it is worth working on problems like inverse rendering and material segmentation, as it can find a large variety of applications in the real world, even beyond the computer science community.